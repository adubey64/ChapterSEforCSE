\label{sec:vandv}

Scientitic software is designed to model some phenomena in the
physical world. The phenomena may be at microscopic level, for example
protein folding, or at really large scales, for example galaxy cluster
mergers. We use the term physical to also include chemical and
biological systems since physical processes are underlying building
blocks for all those systems also. 
Their physical characteristics are translated into
mathematical models that are said to describe the essential features
of the behaviour of the physical system being studied. In order to use
digital computers to run the models it is necessary to discretize the
equations of the mathematical model and devise and implement numerical
algorithms to solve them. Thus there are many stages in the process
where errors can be introduced in the process. Classes of these errors
are mostly orthogonal to one another, and good verification and
validation methodology reflects that. The terms verification and
validation are used in ways where they don't always mean the same
thing. In one narrow definition of validation, it ensures that the mathematical
model is correctly defining the physical phenomena, while verification
makes sure that the implementation of the model is correct. In other
words, model is validated against observations or experiments from the
physical world, whereas verification encompasses all other 
testing.   Other definitions give broader scope to validation. For
example, validation can also apply to a numerical method through
exercises such as code-to-code comparisons, and its order can be
validated through convergence studies. Similarly the implementation of
a solver can be validated against an analytically obtained solution
for a simplifer application of the same solver. Irrespective to the
specific defintions, what is true is that correctness must be assured
at all the stages from model to implementation. In the context of CSE
applications correctness really implies acceptable, since exact
solutions cannot be computed for reasons below. 

The modeling process starts with the equations that describe the
general class of behavior to be studies, for example the Navier-Stokes
equations describe the flow of compressible and incompressible
fluids. There may be more equations if there are other features in the
behavior that are not adequately captured by one equation, or terms
may be added to the equation. In translating the model from
mathematical representation to computational representation two
processes go on simultaneously. One is the discretization, and the
second one is approximation. One can argue that discretization is by
definition an approximate process because it is in effect sampling the
continuous behavior, and therefore has an error term that is ignored
by the implementation. Contrary to the intuitive assumption that
minimizing error term is always desirable, it is not always so because
of finite machine precision \cite{}. But error terms are not the only
approximations. Depending upon the level of understanding of speicific
sub-phenomena, and available compute resources, scientists also 
use judgement to make other approximations. Sometimes, to focus on a
particular behavior, a term may be simplified or may be even completely
dropped from the equation. At other times some physical details may be dropped
from the model because they are not understood well enough by the
scientists. Because of these many degrees of freedom in the modeling
process itself, the validation of the model also has to be very
carefully caliberated by the scientists themselves. This is an
instance of particular challenges in the CSE software where no amount
of specification is enough to hand the implementation over to the
software engineers/developers. A close collaboration is must because
the process has to be iterative and scientific judgement has to be
applied. 





 
