\label{sec:lifecycle} 
Scientific software is designed to model some phenomena in the
physical world. The phenomena may be at microscopic level, for example
protein folding, or at extremely large scales, for example galaxy cluster
mergers.  In some applications multiple scales are modeled.  (The term 'physical' used here includes chemical and
biological systems since physical processes are underlying building
blocks for those systems too.) The physical characteristics of the systems being modeled are
translated into mathematical models that are said to describe the
essential features of the behavior of the system being
studied. These equations are then discretized, and numerical algorithms
are used to solve them. This process requires diverse expertise and
adds many stages in the development and lifecycle of scientific
software that may not be encountered elsewhere. 
\comment{KA NOTE: Do we have any thing to support this claim?  If not
  I would just delete the sentence as it doesn't help our argument.} 
\response{AD: Does that read better ?}

\subsection{Development Cycle}
\label{sec:dev-cycle}
For scientific simulations, modeling begins with equations that describe the
general class of behavior to be studied, for example the Navier-Stokes
equations describe the flow of compressible and incompressible
fluids, and Van-der-vaal equations describe force interactions among
molecules in a material. There may be more than one set of equations
if there are behaviors that are not adequately captured by one set.
In translating the model from mathematical representation to
computational representation two processes go on simultaneously,
discretization and approximation. One can argue that discretization is,
by definition, an approximation because it is in effect sampling
continuous behavior where information is lost between sampling
intervals. This loss manifests itself as error terms in the discretized
equations, but error terms are not the only
approximations. Depending upon the level of understanding of specific
sub-phenomena, and available compute resources, scientists also 
use their judgement to make other approximations. Sometimes, to focus on a
particular behavior, a term in an equation may be simplified or may be even completely
dropped. At other times some physical details may be dropped
from the model because they are not understood well enough by the
scientists.  Or the model itself may be an approximation.  

The next stage in developing the code is finding the appropriate
numerical methods for each of the models. Sometimes good methods exists that
can be used ''as-is''.  Other times, they may need to be customized, or new
methods may need to be developed. There may need to be validation of
the method's applicability to the model if the method is new or
significantly modified. Unless an implementation of the method is
readily available as a third party software (stand-alone or in a
library), it has to be implemented and verified. It is at this stage
that the development of a CSE code begins to resemble that of general
software. The numerical algorithms are specified, the semantics are
understood, and they need to be translated into executable
code. Figure \ref{Fig:dev-cycle} gives an example of the development
cycle of a multiphysics application modeled using partial differential
equations. 

\begin{figure}[!t]
\centering
\includegraphics[ width=4.0in]{CSE-design}
\vskip -0.25in
\caption{Development cycle of modeling with partial differential equations}
\label{Fig:dev-cycle}
\end{figure}



\subsection{Verification and Validation}
\label{sec:vandv}
% There are many stages in the development cycle of scientific software 
% where errors can be introduced. 
% Many of the errors introduced in one
% stage have no correlation with those in other stages. A good verification and validation
% methodology will exploit this knowledge .  
\comment{KA NOTE: I don't know what the last sentence means...  can
  you expand a bit?  It pretty vague.}
\response_AD: Upon re-reading it seemed redundant, so I removed the
whole argument}
The terms verification and
validation are often used interchangeably, but to some communities have specific definitions.  
In one narrow definition, validation, ensures that the
mathematical model correctly defines the physical phenomena, while
verification makes sure that the implementation of the model is
correct. In other words, a model is validated against observations or
experiments from the physical world, whereas a model is verified by
other forms of testing.   Other definitions give broader scope to 
validation. For example, validation of a numerical
method may be constructed through code-to-code comparisons, and its
order can be validated through convergence studies. Similarly, the
implementation of a solver can be validated against an analytically
obtained solution for some model if the same solver can be
applied and the analytical solution is also known, though this is not
always possible.  Irrespective of  specific definitions, what is true is that
correctness must be assured at all the stages from model to
implementation.  

There are many degrees of freedom in the process of deriving a
model as discussed in the previous section, therefore, the validation of the
model must be carefully calibrated by scientific experts. Similarly,
verification of a numerical method requires applied math expertise
because the method needs verification of its stability, accuracy and
order of convergence, in addition to correctness. Numerical methods
have their own error analysis because of approximations and many of
these methods are themselves objects of ongoing research, so their
implementation may need modifications from time to time. Whenever
this happens, the entire gamut of verification and validation needs to
be applied again. This is an instance of a particular challenge in the
CSE software where no amount of specification is enough to hand the
implementation over to software engineers or developers who do not have domain or method knowledge. A close
collaboration with applied mathematicians and method developers is necessary because the process has to be iterative with
scientific judgement applied at every iteration. 

One other unique verification challenge in CSE software is the
consequences of finite machine precision of floating point
numbers. Any change in compilers, optimization levels, and even order
of operations can cause numerical drift in the solutions. Especially
in applications that have a large range of scales, it can be extremely
difficult to differentiate between a legitimate bug and a numerical
drift. Therefore, relying upon bitwise reproducibility of the solution is
rarely a sufficient method for verifying the continued correctness of
an application behavior. Robust diagnostics (such as statistics or
conservation of physical quantities) need to be built into the
verification process.  This issue is
discussed in greater detail in chapter \ref{chp:testing}.

\subsection{Maintenance and Extensions}
\label{sec:maintain}
In a simplified view of software lifecycle, there is a design and development phase,
followed by production and maintenance phase. \comment{KA NOTE:  I'm
  not sure we want to make this claim.  I've softened it a bit.
  Commercial codes are constantly releasing new features....}
\response{AD: OK, I have further diluted it}   Even in well engineered
codes this simplified view typically applies only to the
infrastructure and API's which have a distinct development phase which
has limited spill into the remainder of the lifecycle. The numerical
algorithms and solvers can be in a continually evolving state
reflecting the advances in their respective fields.  
\comment{KA NOTE: NEED REF - can we claim 'most successful' here?} 
\response{AD: see if you like the revised version'}
The development of CSE software is
usually responding to an immediate scientific need, so the codes get
employed in production as soon as a minimal set of computational
modules necessary for even one scientific project are
built. Similarly, the development of computational modules almost
never stops all through the code lifecycle because new findings in science
and math almost continuously place new demands on them. The additions
are mostly incremental when they incorporate new findings into an
existing feature, they can also be substantial when new capabilities
are added. The need for new capabilities may arise from 
greater model fidelity, or from trying to simulate a more complex
model. Sometimes a code designed for one scientific field may have
enough in common with another field that capabilities may be added to
enable it for the new field.   

Whatever may be the cause, co-existence of development and
production/maintenance phases is a constant challenge to the code
teams. It becomes acute when the code needs to undergo major version
changes. The former can be managed with some repository
discipline in the team coupled with a solid testing regime. The latter
is a much bigger challenge where the plan has to concern itself with
questions such as how much backward compatibility is suitable, how
much code can go offline, and how to reconcile ongoing development in
code sections that are substantially different between versions.
FLASH's example in section \ref{sec:FLASHSoftwareProcess} describes
a couple of strategies that met the conflicting needs of developers and
production users in both scenarios. Both required co-operation and
buy-in from all the stakeholders to be successful. 

\subsection{Using CSE Software}
\input{using}
